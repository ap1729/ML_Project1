{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FTTZFGCB-iY2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HcRyWDF-iZA"
   },
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "id": "9bKKtSfY-iZC",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "DATA_TRAIN_PATH = 'train.csv' \n",
    "y_orig, tX_raw, ids = load_csv_data(DATA_TRAIN_PATH) #Standardized data ready to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN FOR 4 WAY SPLIT ALGORITHM\n",
    "# Run to standardize data, mode = 0, 1, 2\n",
    "# mode 0: basic standardization with standard deviation\n",
    "# mode 1: quantile standardization\n",
    "# mode 2: scaled standardization\n",
    "tX_raw = standardize_data(tX_raw, mode = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DVyUVVCE-iZF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 121)\n"
     ]
    }
   ],
   "source": [
    "degree=1 #degree atleast 1\n",
    "flags =[1,1,0] #Log,sin,cos flags \n",
    "tX_orig = feature_expand(tX_raw,degree,flags)\n",
    "print(tX_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV6cs-7W-iZG",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xGwsx4xl-iZH"
   },
   "outputs": [],
   "source": [
    "i1 = 10 # Fix one index to analyze \n",
    "tXi1= [row[i1] for row in tX_orig]\n",
    "print(len(tXi1),len(y_orig))\n",
    "plt.figure()\n",
    "plt.plot(tXi1,y_orig,'bo')\n",
    "plt.xlabel(i1)\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "#Plotting all graphs across various indices versus chosen index to see dependence\n",
    "for i2 in range(0,31):\n",
    "        plt.figure()\n",
    "        tXi2= [row[i2] for row in tX_orig]\n",
    "        plt.plot(tXi1, tXi2,'ro')\n",
    "        plt.xlabel(i1)\n",
    "        plt.ylabel(i2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sbkr6Lf-iZI",
    "tags": []
   },
   "source": [
    "## Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pBjTog0h-iZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242500,) (242500, 30) (7500,) (7500, 30)\n"
     ]
    }
   ],
   "source": [
    "#Split Training Data into 2 parts one for training and other for cross-validating with a fixed seed\n",
    "\n",
    "ratio=0.97 #percentage of data to be trained and what to be used for cros-validating\n",
    "seed=13\n",
    "\n",
    "tX,tX_cross,y,y_cross = split_data(tX_raw,y_orig,ratio,seed)\n",
    "\n",
    "print(y.shape,tX.shape,y_cross.shape,tX_cross.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zow8jXc8-iZL",
    "tags": []
   },
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn2YdM0Q-iZL"
   },
   "outputs": [],
   "source": [
    "print([row[23] for row in tX[0:10]])  #23rd column is the quarternary one PRI_jet_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9iwW_WC-iZM",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ug7mGZlY-iZN"
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.1\n",
    "\n",
    "\n",
    "#tX=np.c_[np.ones(len(y)), tX[:, 14:31 ] ]  # to comment this line for only primitive values\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "\n",
    "weights = gradient_ws[-1]\n",
    "loss=gradient_losses[-1]\n",
    "print(weights)\n",
    "#weights= np.array([weights[0],0,0,0,0,0,0,0,0,0,0,0,0,0]  +list(weights[1:])) # to comment this line for only primitive values\n",
    "#print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izm-jPqT-iZN",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLyRRVqa-iZO"
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "max_iters = int(len(y)/batch_size)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "\n",
    "# Start gradient descent.\n",
    "stoch_gradient_losses, stoch_gradient_ws = stochastic_gradient_descent(y, tX, w_initial, batch_size, max_iters, gamma)\n",
    "\n",
    "weights = stoch_gradient_ws[-1]\n",
    "loss=stoch_gradient_losses[-1]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiG6-_Ez-iZO",
    "tags": []
   },
   "source": [
    "## Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gvIB7npS-iZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.94192259e-02 -2.52518124e-01 -2.54701272e-01 -3.04953456e-02\n",
      " -1.41582139e+00  2.94531395e-01 -1.07763775e+01  2.68141936e-01\n",
      " -2.80309591e-03 -3.27450781e+02 -1.82440637e-01  1.13314114e-01\n",
      "  2.05970765e+01  6.36184284e+01  5.60971571e-05 -1.79410494e-03\n",
      "  6.27314892e+01 -6.32998485e-04  1.65431667e-03  1.21838797e-01\n",
      "  7.25046202e-04 -6.36012656e-02 -2.04984811e-01 -1.03064688e-01\n",
      "  2.17324634e-02  2.31403894e-01 -4.30612487e-02 -3.05927893e+00\n",
      " -5.36893579e+00  2.77312025e+02]\n",
      "0.38915286643055325\n"
     ]
    }
   ],
   "source": [
    "loss,weights = least_squares(y,tX)\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRP0k8v8-iZP",
    "tags": []
   },
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hDBX9Pj-iZP"
   },
   "outputs": [],
   "source": [
    "#Find best lambda value for ridge regression\n",
    "\n",
    "#selecting values of lambda to test\n",
    "n_lambdas = 50\n",
    "lambdas = np.logspace(-7, 0, n_lambdas)\n",
    "\n",
    "#using polynomial of degree to determine basis functions to fit nonlinear data\n",
    "\n",
    "txtrn = tX #comment this line and uncomment above for polynomial fitting\n",
    "\n",
    "rmse_lst=[]\n",
    "rmse_min=1e10\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "\n",
    "    rmse, weights_trn = ridge_regression(y, txtrn, lambda_)\n",
    "    rmse_lst.append(rmse)\n",
    "    \n",
    "    if rmse < rmse_min:\n",
    "        rmse_min = rmse\n",
    "        lambda_opt = lambda_\n",
    "        weights = weights_trn\n",
    "        \n",
    "plt.semilogx(lambdas, rmse_lst, color='r', marker='*', label=\"RMSE\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Ridge regression\")\n",
    "leg = plt.legend(loc=1, shadow=True)\n",
    "leg.draw_frame(False)\n",
    "\n",
    "print(\"The best lambda value for ridge regression is \",lambda_opt)\n",
    "print(\"Least mse is \", mse_min) \n",
    "print(\"Weights \", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1CAYjs3-iZQ",
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmACqMmR-iZS"
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.000002\n",
    "# Initialization\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "y[np.where(y==-1)]=0 # Since y is array with -1 and +1 , we need to make it to 0's and 1's\n",
    "\n",
    "# Start Logistic gradient descent.\n",
    "logistic_losses, logistic_ws = logistic_gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "\n",
    "weights = logistic_ws[-1]\n",
    "loss=logistic_losses[-1]\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dMCGHOH-iZT",
    "tags": []
   },
   "source": [
    "## Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUs6Ltt8-iZU"
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.000001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "y[np.where(y==-1)]=0 # Since y is array with -1 and +1 , we need to make it to 0's and 1's\n",
    "\n",
    "\n",
    "'''Loop for Lambda and find best value  '''\n",
    "lambdas = np.logspace(-4, 0, 10)\n",
    "loss=1e10\n",
    "\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    # Start Regularized gradient descent.\n",
    "    reg_logistic_losses_temp, reg_logistic_ws_temp = reg_logistic_gradient_descent(y, tX, w_initial, max_iters, gamma,lambda_)\n",
    "    \n",
    "    print(lambda_, reg_logistic_losses_temp[-1])\n",
    "    if loss>reg_logistic_losses_temp[-1]:\n",
    "        loss = reg_logistic_losses_temp[-1]\n",
    "        weights= reg_logistic_ws_temp[-1]\n",
    "        lambda_opt = lambda_\n",
    "\n",
    "print(\"The best lambda value for regularized logistic regression is \",lambda_opt)\n",
    "print(weights)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbxjvRjy-iZV",
    "tags": []
   },
   "source": [
    "## Newton Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjmyTK0R-iZW"
   },
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 300\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "y[np.where(y==-1)]=0 # Since y is array with -1 and +1 , we need to make it to 0's and 1's\n",
    "\n",
    "''' TODO Hessian computation is too large''' \n",
    "''' Maybe combine with SGD'''\n",
    "''' DONT RUN ''''\n",
    "\n",
    "\n",
    "# Start Newton logistic gradient descent.\n",
    "newt_logistic_losses, newt_logistic_ws = newt_logistic_gradient_descent(y, tX, w_initial, max_iters, gamma)\n",
    "\n",
    "weights = logistic_ws[-1]\n",
    "loss=logistic_losses[-1]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Newton Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "\n",
    "gamma = 0.0001\n",
    "batch_size = 1000\n",
    "max_iters = int(len(y)/batch_size)\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(len(tX[0]))\n",
    "y[np.where(y==-1)]=0 # Since y is array with -1 and +1 , we need to make it to 0's and 1's\n",
    "\n",
    "# Start Newton Stochastic gradient descent\n",
    "newt_stochastic_losses, newt_stochastic_ws = newt_stochastic_gradient_descent(y, tX, w_initial, max_iters, gamma, batch_size)\n",
    "\n",
    "weights = newt_stochastic_ws[-1]\n",
    "loss=newt_stochastic_losses[-1]\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HGcva-V-iZW",
    "tags": []
   },
   "source": [
    "## Fancy Idea No.1 : Split Data into 4 parts and run GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j3x94Z1-iZW",
    "outputId": "97a481ba-a04c-4fd3-ad61-4c9f40155c51"
   },
   "outputs": [],
   "source": [
    "#NOTE: ONLY RUN FIRST TWO CELLS BEFORE THIS, ONLY LOAD DATA\n",
    "# DONT RUN FEATURE EXPANSION OR CROSS VALIDATION, DOING IT HERE\n",
    "tX23=[row[22] for row in X_raw[0:100]] #23rd column which is JET_PRI_NUM and has exactly 4 distinct values\n",
    "values=(list(set(tX23))) #get all 4 unique values\n",
    "indices=[]\n",
    "\n",
    "weights_new = []\n",
    "loss_new   = []\n",
    "\n",
    "#Split each of the 4 subsets of Training Data into 2 parts one- for training and other for cross-validating with a fixed seed\n",
    "\n",
    "#Cross Validation Parameters\n",
    "ratio=0.97 #percentage of data to be trained and what to be used for cros-validating\n",
    "seed=13 #random seed\n",
    "\n",
    "#Feature Expansion Parameters\n",
    "degree=1 #degree atleast 1\n",
    "\n",
    "#Logistic Regression Parameters\n",
    "gamma=[0.000002,0.0000055,0.0000054,0.0000056]\n",
    "flags = [[1,1,1],[0,1,1],[0,1,1],[0,1,1] ]\n",
    "max_iters = 1000 \n",
    "\n",
    "cross_val= [0,0,0,0] #cross validation score for each group\n",
    "shapes=[]\n",
    "\n",
    "# Replace every -999. in the first column by the median of the said column to avoid having it deleted\n",
    "\n",
    "first_col = np.array(X_raw[:,0])\n",
    "print(first_col)\n",
    "first_col[first_col == -999.] = 0\n",
    "first_col[first_col == 0.] = np.median(first_col)\n",
    "X_raw[:,0] = first_col\n",
    "\n",
    "col_to_del = []\n",
    "\n",
    "#X_reduced, y_reduced = clean_columns_rows(X_raw, y_orig, 0, 1, [1])\n",
    "#tX_reduced = standardize_data(X_reduced, mode = 0)\n",
    "\n",
    "for i in range(0,4): #iterate over each of the 4 possibilites of pri num\n",
    "    curr_indices = []\n",
    "    curr_indices=np.where( [row[22] for row in X_raw] ==values[i])[0] \n",
    "    indices.append(curr_indices) #keeps track of indices\n",
    "\n",
    "    X_reduced, y_reduced, del_col = clean_columns_rows(X_raw[curr_indices,:], y_orig[curr_indices], 0, 1, flags[i])\n",
    "\n",
    "    #Standardize all the remaining columns\n",
    "    X_reduced = standardize_data(X_reduced, mode = 1)\n",
    "\n",
    "    tX_4way_temp = feature_expand(X_reduced,degree,flags[i]) #Feature Expansion\n",
    "    y_4way_temp = y_reduced\n",
    "\n",
    "    #Cross validation split    \n",
    "    tX_4way_train,tX_4way_cross,y_4way_train,y_4way_cross = split_data(tX_4way_temp,y_4way_temp,ratio,seed)\n",
    "    \n",
    "    shapes.append(y_4way_train.shape[0])\n",
    "\n",
    "    #Running Logistic Regression ***********************\n",
    "\n",
    "\n",
    "    # Initialization\n",
    "    w_initial = np.zeros(len(tX_4way_train[0]))\n",
    "    y_4way_train[np.where(y_4way_train==-1)]=0 # Since y is array with -1 and +1 , we need to make it to 0's and 1's\n",
    "\n",
    "\n",
    "    # Start Logistic gradient descent\n",
    "    logistic_losses, logistic_ws = logistic_gradient_descent(y_4way_train, tX_4way_train, w_initial, max_iters, gamma[i])\n",
    "\n",
    "    weights_new.append(logistic_ws[-1])\n",
    "    loss_new.append(logistic_losses[-1])\n",
    "    \n",
    "    cross_val[i] = cross_validation(logistic_ws[-1],tX_4way_cross,y_4way_cross)\n",
    "\n",
    "    col_to_del.append(del_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKIkGowt-iZY"
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUDXadf0-iZZ"
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'output.csv' \n",
    "\n",
    "y_pred = predict_labels_new(weights_new, X_test,degree,flags, col_to_del)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohTxXab1-iZZ",
    "tags": []
   },
   "source": [
    "## Cross Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XbkIiMBX-iZa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score :  0.7209333333333333\n"
     ]
    }
   ],
   "source": [
    "#Split original data used above into two parts and cross validate here to get expected accuracy\n",
    "print(\"Cross Validation Score : \",cross_validation(weights,tX_cross,y_cross))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I3-eigK-iZa",
    "tags": []
   },
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "id": "zZ6YZtsE-iZb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "id": "igsnoGps-iZb",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ML_course/projects/project1/scripts/implementations.py:322: RuntimeWarning: divide by zero encountered in log\n",
      "  tx=np.c_[tx,np.log(np.abs(x))]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (568238,121) and (30,) not aligned: 121 (dim 1) != 30 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_778/1391248135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtX_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ML_course/projects/project1/scripts/implementations.py\u001b[0m in \u001b[0;36mpredict_labels\u001b[0;34m(weights, data)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (568238,121) and (30,) not aligned: 121 (dim 1) != 30 (dim 0)"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = 'output.csv' \n",
    "\n",
    "'''NOTE: If Logistic, then the predict labels should be at 0.5 and not 0 since it is a 0-1 problem'''\n",
    "\n",
    "tX_pred=feature_expand(tX_test,degree,flags)\n",
    "y_pred = predict_labels(weights, tX_pred)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pslc1_EH-iZc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUmsgb8_-iZc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
